import{_ as a,o as e,c as t,O as n}from"./chunks/framework.9482e208.js";const o="/assets/watermark.a4dfde82.png",h=JSON.parse('{"title":"sd-panorama","description":"","frontmatter":{},"headers":[],"relativePath":"panorama/index.md","filePath":"panorama/index.md"}'),r={name:"panorama/index.md"},s=n('<h1 id="sd-panorama" tabindex="-1">sd-panorama <a class="header-anchor" href="#sd-panorama" aria-label="Permalink to &quot;sd-panorama&quot;">​</a></h1><p>上面一篇文章中我们着重讲述了如何训练Stable Diffusion中的unet，同时，也了解了在训练好unet之后怎么使用它来去除噪声以及生成图片。然而，还有一个重要的地方我们尚未提及，那就是可控性。我们如何用语言来控制最后生成的结果？答案也很简单——注意力机制。在第一篇文章中我们讲到，一个unet除了要接收噪声图之外，还要接受我们用Text Encoder预先提取的语义信息。那这个语义信息怎么在生成图片的过程中使用呢？我们直接使用注意力机制在unet内层层耦合即可。下图中每个黄色的小方块都代表一次注意力机制的使用，而每次使用注意力机制，就发生了一次图片信息和语义信息的耦合。每一个unet内部，这样的操作都会发生很多次很多次，直到最后一个unet为止，一直不断重复这耦合的过程。</p><p>有人可能就要问了，语义信息是语义信息，图片信息是图片信息，怎么能用attention耦合到一块呢？计算机怎么把这两种完全不同的信息联系到一起的呢？这就要说到我们这篇文章的主角——CLIP模型了。</p><h2 id="clip模型介绍" tabindex="-1">CLIP模型介绍 <a class="header-anchor" href="#clip模型介绍" aria-label="Permalink to &quot;CLIP模型介绍&quot;">​</a></h2><p>自从2018年Bert发布以来，Transformer的语言模型就成了主流。Stable Diffusion起初的版本便是用的基于GPT的CLIP模型，而最近的2.x版本换成了更新更好的OpenCLIP。语言模型的选择直接决定了语义信息的优良与否，而语义信息的好坏又会影响到最后图片的多样性和可控性。Google在Imagen论文中做过实验，可以发现不同语言模型对生成结果的影响是相当大的。</p><p><img src="'+o+'" alt="An image"></p><p>这是一个测试哦</p>',7),i=[s];function p(d,_,c,m,l,u){return e(),t("div",null,i)}const P=a(r,[["render",p]]);export{h as __pageData,P as default};
